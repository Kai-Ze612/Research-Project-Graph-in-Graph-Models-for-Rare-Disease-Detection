{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved data\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_data(filepath):\n",
    "    train_path = os.path.join(filepath, 'train_corrected.pkl')\n",
    "    val_path = os.path.join(filepath, 'val_corrected.pkl')\n",
    "    test_path = os.path.join(filepath, 'test_corrected.pkl')\n",
    "    \n",
    "    with open(train_path, 'rb') as file:   \n",
    "        train_pg_subgraph = pickle.load(file)\n",
    "    \n",
    "    with open(val_path, 'rb') as file:\n",
    "        val_pg_subgraph = pickle.load(file)\n",
    "        \n",
    "    with open(test_path, 'rb') as file:\n",
    "        test_pg_subgraph = pickle.load(file)\n",
    "\n",
    "    return train_pg_subgraph, val_pg_subgraph, test_pg_subgraph\n",
    "\n",
    "file_path = './Data/saved_graphs/corrected_datasets'\n",
    "\n",
    "train_pg_subgraph, val_pg_subgraph, test_pg_subgraph = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique true gene ids is: 2405\n",
      "{4: 0, 10: 1, 8204: 2, 8208: 3, 18: 4, 8210: 5, 22: 6, 23: 7, 29: 8, 30: 9, 8224: 10, 33: 11, 37: 12, 8231: 13, 41: 14, 8234: 15, 43: 16, 42: 17, 8246: 18, 61: 19, 67: 20, 8266: 21, 74: 22, 8268: 23, 78: 24, 80: 25, 83: 26, 85: 27, 87: 28, 88: 29, 8281: 30, 8283: 31, 8284: 32, 92: 33, 95: 34, 97: 35, 98: 36, 100: 37, 111: 38, 8305: 39, 8318: 40, 130: 41, 133: 42, 143: 43, 144: 44, 146: 45, 150: 46, 8345: 47, 154: 48, 162: 49, 163: 50, 57507: 51, 57506: 52, 169: 53, 8362: 54, 173: 55, 177: 56, 178: 57, 8371: 58, 8372: 59, 57521: 60, 8376: 61, 8378: 62, 187: 63, 8379: 64, 8388: 65, 8395: 66, 207: 67, 209: 68, 212: 69, 8405: 70, 213: 71, 8411: 72, 8416: 73, 8422: 74, 232: 75, 8428: 76, 238: 77, 242: 78, 8434: 79, 8437: 80, 246: 81, 8439: 82, 245: 83, 249: 84, 57595: 85, 8446: 86, 256: 87, 8454: 88, 8456: 89, 8458: 90, 8462: 91, 8464: 92, 57618: 93, 8466: 94, 276: 95, 279: 96, 8473: 97, 8477: 98, 8480: 99, 288: 100, 57632: 101, 291: 102, 8483: 103, 8485: 104, 8486: 105, 8484: 106, 296: 107, 8489: 108, 298: 109, 8491: 110, 8487: 111, 297: 112, 8494: 113, 303: 114, 57651: 115, 8501: 116, 8505: 117, 8506: 118, 315: 119, 8514: 120, 8515: 121, 8526: 122, 8527: 123, 337: 124, 339: 125, 341: 126, 344: 127, 349: 128, 8543: 129, 352: 130, 362: 131, 8555: 132, 363: 133, 8566: 134, 375: 135, 8570: 136, 8571: 137, 383: 138, 8578: 139, 387: 140, 388: 141, 8580: 142, 390: 143, 391: 144, 393: 145, 395: 146, 8589: 147, 398: 148, 405: 149, 407: 150, 410: 151, 8604: 152, 8605: 153, 8606: 154, 420: 155, 426: 156, 8620: 157, 429: 158, 428: 159, 431: 160, 440: 161, 8644: 162, 8646: 163, 455: 164, 460: 165, 8656: 166, 8658: 167, 8660: 168, 470: 169, 474: 170, 476: 171, 477: 172, 8671: 173, 481: 174, 487: 175, 489: 176, 490: 177, 492: 178, 8687: 179, 496: 180, 501: 181, 504: 182, 505: 183, 509: 184, 8702: 185, 8709: 186, 8712: 187, 522: 188, 526: 189, 8719: 190, 8725: 191, 535: 192, 540: 193, 543: 194, 544: 195, 8737: 196, 553: 197, 8746: 198, 8747: 199, 554: 200, 560: 201, 8754: 202, 8764: 203, 574: 204, 576: 205, 577: 206, 582: 207, 8775: 208, 591: 209, 594: 210, 602: 211, 603: 212, 608: 213, 8801: 214, 611: 215, 8805: 216, 8806: 217, 8810: 218, 8817: 219, 625: 220, 8819: 221, 628: 222, 8821: 223, 8823: 224, 8824: 225, 642: 226, 643: 227, 656: 228, 8854: 229, 8855: 230, 665: 231, 666: 232, 668: 233, 8860: 234, 8864: 235, 8869: 236, 8871: 237, 680: 238, 679: 239, 682: 240, 8875: 241, 691: 242, 8885: 243, 695: 244, 702: 245, 708: 246, 711: 247, 8905: 248, 714: 249, 715: 250, 8908: 251, 8906: 252, 721: 253, 8919: 254, 727: 255, 729: 256, 735: 257, 742: 258, 743: 259, 8936: 260, 751: 261, 8946: 262, 754: 263, 756: 264, 762: 265, 769: 266, 775: 267, 780: 268, 789: 269, 790: 270, 8983: 271, 791: 272, 8986: 273, 8987: 274, 795: 275, 794: 276, 802: 277, 803: 278, 8995: 279, 9001: 280, 9005: 281, 9010: 282, 9020: 283, 9024: 284, 9025: 285, 58178: 286, 839: 287, 841: 288, 847: 289, 852: 290, 855: 291, 857: 292, 858: 293, 862: 294, 863: 295, 867: 296, 9064: 297, 874: 298, 9066: 299, 9069: 300, 878: 301, 880: 302, 9073: 303, 9079: 304, 888: 305, 9081: 306, 891: 307, 9091: 308, 906: 309, 909: 310, 9106: 311, 916: 312, 919: 313, 9116: 314, 927: 315, 929: 316, 934: 317, 935: 318, 940: 319, 9135: 320, 9137: 321, 948: 322, 955: 323, 9149: 324, 9152: 325, 9154: 326, 9155: 327, 9161: 328, 9165: 329, 981: 330, 988: 331, 989: 332, 9184: 333, 995: 334, 996: 335, 58342: 336, 1000: 337, 58348: 338, 1013: 339, 1014: 340, 9210: 341, 1020: 342, 9216: 343, 9219: 344, 9220: 345, 1029: 346, 58375: 347, 9228: 348, 1040: 349, 9233: 350, 9237: 351, 1046: 352, 9238: 353, 1049: 354, 9244: 355, 9246: 356, 1057: 357, 9256: 358, 1066: 359, 1069: 360, 9264: 361, 1073: 362, 1072: 363, 9268: 364, 1082: 365, 1085: 366, 1086: 367, 9280: 368, 9282: 369, 1093: 370, 9286: 371, 1098: 372, 9299: 373, 9300: 374, 1109: 375, 9306: 376, 1116: 377, 9309: 378, 9312: 379, 1120: 380, 1124: 381, 9316: 382, 1126: 383, 1130: 384, 1131: 385, 1132: 386, 9323: 387, 1135: 388, 9330: 389, 1148: 390, 9341: 391, 1151: 392, 9343: 393, 9345: 394, 1153: 395, 1154: 396, 9350: 397, 9352: 398, 9353: 399, 1164: 400, 1165: 401, 1173: 402, 1175: 403, 9370: 404, 9371: 405, 1181: 406, 1188: 407, 9382: 408, 1193: 409, 9388: 410, 1198: 411, 9391: 412, 1204: 413, 1205: 414, 9397: 415, 1210: 416, 9403: 417, 1216: 418, 9408: 419, 9410: 420, 1221: 421, 1222: 422, 1224: 423, 1229: 424, 9424: 425, 1235: 426, 1239: 427, 9435: 428, 9438: 429, 9441: 430, 1251: 431, 1253: 432, 1255: 433, 1263: 434, 9459: 435, 1270: 436, 9466: 437, 9469: 438, 9471: 439, 9475: 440, 9487: 441, 1296: 442, 9489: 443, 1298: 444, 1300: 445, 1301: 446, 1309: 447, 1311: 448, 1312: 449, 1314: 450, 1340: 451, 1344: 452, 1350: 453, 1362: 454, 1365: 455, 1366: 456, 1367: 457, 9558: 458, 9562: 459, 9563: 460, 1371: 461, 1373: 462, 1374: 463, 1378: 464, 1379: 465, 1383: 466, 1389: 467, 9589: 468, 9604: 469, 1414: 470, 9613: 471, 9631: 472, 1448: 473, 1455: 474, 9652: 475, 9655: 476, 1464: 477, 1466: 478, 1471: 479, 9667: 480, 1476: 481, 9673: 482, 9676: 483, 1485: 484, 9683: 485, 1493: 486, 1495: 487, 9691: 488, 1500: 489, 9695: 490, 1515: 491, 9711: 492, 1520: 493, 9714: 494, 1523: 495, 1522: 496, 9718: 497, 1526: 498, 9720: 499, 9721: 500, 1527: 501, 1531: 502, 9724: 503, 1535: 504, 1537: 505, 1553: 506, 9746: 507, 9747: 508, 9748: 509, 1557: 510, 1558: 511, 1559: 512, 9754: 513, 1564: 514, 9767: 515, 9778: 516, 1587: 517, 1593: 518, 1600: 519, 1604: 520, 1606: 521, 9800: 522, 1611: 523, 9807: 524, 1618: 525, 9813: 526, 9814: 527, 1622: 528, 1626: 529, 9824: 530, 1633: 531, 9826: 532, 1632: 533, 9829: 534, 9831: 535, 1639: 536, 1641: 537, 9833: 538, 1640: 539, 9835: 540, 1645: 541, 1648: 542, 1650: 543, 1651: 544, 1653: 545, 9855: 546, 1666: 547, 1682: 548, 9875: 549, 1684: 550, 1685: 551, 1687: 552, 1691: 553, 1692: 554, 9891: 555, 1706: 556, 1707: 557, 1709: 558, 9901: 559, 1710: 560, 9905: 561, 1715: 562, 8482: 563, 1721: 564, 1722: 565, 9915: 566, 1724: 567, 1725: 568, 1738: 569, 1739: 570, 9933: 571, 1742: 572, 1741: 573, 1744: 574, 1746: 575, 9938: 576, 1748: 577, 1756: 578, 1757: 579, 1760: 580, 1761: 581, 9954: 582, 1762: 583, 9956: 584, 9965: 585, 1774: 586, 9967: 587, 1775: 588, 9970: 589, 1781: 590, 9975: 591, 1785: 592, 9978: 593, 9986: 594, 9987: 595, 9988: 596, 1807: 597, 1811: 598, 1825: 599, 10018: 600, 1831: 601, 10027: 602, 10032: 603, 10034: 604, 10037: 605, 10041: 606, 1852: 607, 1863: 608, 1866: 609, 1868: 610, 1873: 611, 1874: 612, 1876: 613, 10074: 614, 10075: 615, 1885: 616, 1887: 617, 10084: 618, 1894: 619, 1897: 620, 1899: 621, 10093: 622, 10094: 623, 1902: 624, 1904: 625, 1905: 626, 1910: 627, 1920: 628, 1921: 629, 1928: 630, 10123: 631, 1931: 632, 1934: 633, 1935: 634, 1939: 635, 10132: 636, 1942: 637, 10144: 638, 1955: 639, 1961: 640, 1968: 641, 10161: 642, 1974: 643, 10167: 644, 1977: 645, 1978: 646, 10169: 647, 1980: 648, 1983: 649, 10176: 650, 1987: 651, 10180: 652, 10181: 653, 10183: 654, 1995: 655, 1996: 656, 10197: 657, 2005: 658, 10198: 659, 2017: 660, 2024: 661, 2027: 662, 10223: 663, 2032: 664, 10225: 665, 2037: 666, 10236: 667, 10241: 668, 2055: 669, 2057: 670, 10250: 671, 2059: 672, 10253: 673, 10260: 674, 2069: 675, 2070: 676, 10263: 677, 10265: 678, 2075: 679, 10270: 680, 2083: 681, 10277: 682, 10278: 683, 2087: 684, 10282: 685, 2092: 686, 2096: 687, 10289: 688, 2098: 689, 2099: 690, 10290: 691, 10293: 692, 10295: 693, 2104: 694, 2103: 695, 10299: 696, 2108: 697, 10300: 698, 10303: 699, 2112: 700, 2111: 701, 2117: 702, 2120: 703, 2123: 704, 2128: 705, 2129: 706, 2132: 707, 2134: 708, 2137: 709, 10329: 710, 2140: 711, 2148: 712, 2157: 713, 2158: 714, 2160: 715, 10352: 716, 10356: 717, 2164: 718, 10358: 719, 70444: 720, 2169: 721, 2174: 722, 2175: 723, 10367: 724, 10370: 725, 10371: 726, 10372: 727, 2181: 728, 2182: 729, 10385: 730, 10386: 731, 2198: 732, 2199: 733, 2205: 734, 2213: 735, 2217: 736, 10413: 737, 2221: 738, 10416: 739, 10417: 740, 2228: 741, 10422: 742, 2231: 743, 10424: 744, 2234: 745, 10428: 746, 10429: 747, 10431: 748, 2244: 749, 2249: 750, 70461: 751, 2267: 752, 10459: 753, 10464: 754, 10465: 755, 2274: 756, 2276: 757, 2278: 758, 2286: 759, 2289: 760, 2290: 761, 10485: 762, 2298: 763, 10492: 764, 10495: 765, 2308: 766, 2310: 767, 2311: 768, 2314: 769, 2316: 770, 2320: 771, 2324: 772, 10517: 773, 10521: 774, 2330: 775, 2331: 776, 2332: 777, 10524: 778, 2337: 779, 10531: 780, 2341: 781, 2345: 782, 2347: 783, 2348: 784, 10542: 785, 10546: 786, 10550: 787, 2361: 788, 10559: 789, 10563: 790, 10566: 791, 2382: 792, 2383: 793, 2389: 794, 10582: 795, 2399: 796, 2400: 797, 10593: 798, 10594: 799, 10597: 800, 2406: 801, 2408: 802, 10602: 803, 2411: 804, 2417: 805, 10613: 806, 2423: 807, 2427: 808, 2432: 809, 2434: 810, 2448: 811, 2452: 812, 2453: 813, 10647: 814, 10649: 815, 10654: 816, 2465: 817, 10664: 818, 2473: 819, 2475: 820, 10667: 821, 2481: 822, 2491: 823, 2497: 824, 2498: 825, 10691: 826, 2500: 827, 2501: 828, 10695: 829, 10696: 830, 2504: 831, 2505: 832, 10699: 833, 2509: 834, 2511: 835, 2515: 836, 2526: 837, 10725: 838, 2542: 839, 2543: 840, 10736: 841, 2545: 842, 10740: 843, 10754: 844, 10756: 845, 2569: 846, 2572: 847, 10769: 848, 2580: 849, 2581: 850, 10774: 851, 2585: 852, 2587: 853, 2593: 854, 10788: 855, 10798: 856, 10802: 857, 2616: 858, 2621: 859, 2622: 860, 10818: 861, 2630: 862, 2631: 863, 10824: 864, 2633: 865, 10827: 866, 59982: 867, 2639: 868, 10836: 869, 10837: 870, 2646: 871, 10847: 872, 2663: 873, 2664: 874, 2666: 875, 2667: 876, 2671: 877, 10865: 878, 2674: 879, 10867: 880, 2673: 881, 2677: 882, 2679: 883, 10873: 884, 10874: 885, 10878: 886, 2693: 887, 2698: 888, 2699: 889, 2700: 890, 10894: 891, 60049: 892, 60052: 893, 10900: 894, 2710: 895, 2711: 896, 10902: 897, 2714: 898, 2716: 899, 2719: 900, 51878: 901, 10923: 902, 51885: 903, 10930: 904, 2742: 905, 60087: 906, 2744: 907, 51897: 908, 2746: 909, 51894: 910, 2743: 911, 51902: 912, 51903: 913, 60095: 914, 10944: 915, 2754: 916, 10946: 917, 2756: 918, 60099: 919, 2759: 920, 2767: 921, 2768: 922, 2788: 923, 51945: 924, 2794: 925, 2795: 926, 10988: 927, 51949: 928, 51948: 929, 2799: 930, 10985: 931, 51947: 932, 2803: 933, 11002: 934, 2810: 935, 51965: 936, 2814: 937, 11008: 938, 2817: 939, 2821: 940, 2824: 941, 11030: 942, 2840: 943, 11034: 944, 51997: 945, 11038: 946, 52002: 947, 2853: 948, 11046: 949, 52007: 950, 52008: 951, 11051: 952, 2860: 953, 2859: 954, 52015: 955, 11055: 956, 11057: 957, 2864: 958, 52020: 959, 2873: 960, 52026: 961, 52029: 962, 11076: 963, 2889: 964, 2893: 965, 2895: 966, 52048: 967, 52047: 968, 2898: 969, 2903: 970, 52056: 971, 2905: 972, 2907: 973, 11101: 974, 11104: 975, 2914: 976, 11107: 977, 2916: 978, 52069: 979, 52073: 980, 11114: 981, 52076: 982, 2927: 983, 11120: 984, 11121: 985, 11124: 986, 2933: 987, 2936: 988, 11130: 989, 11134: 990, 52094: 991, 52096: 992, 52097: 993, 2943: 994, 11136: 995, 11135: 996, 2942: 997, 11155: 998, 2963: 999, 11156: 1000, 2970: 1001, 52125: 1002, 52127: 1003, 11172: 1004, 2984: 1005, 52139: 1006, 11180: 1007, 2993: 1008, 3000: 1009, 11193: 1010, 11192: 1011, 52162: 1012, 3011: 1013, 3012: 1014, 11207: 1015, 11209: 1016, 11214: 1017, 52174: 1018, 3025: 1019, 11220: 1020, 3032: 1021, 3035: 1022, 3036: 1023, 11229: 1024, 3038: 1025, 52193: 1026, 3042: 1027, 3043: 1028, 52197: 1029, 3045: 1030, 11238: 1031, 3047: 1032, 52203: 1033, 3052: 1034, 3053: 1035, 11247: 1036, 3056: 1037, 3057: 1038, 3065: 1039, 3073: 1040, 11270: 1041, 11273: 1042, 3085: 1043, 3088: 1044, 52241: 1045, 11283: 1046, 11286: 1047, 3099: 1048, 52254: 1049, 3104: 1050, 3106: 1051, 52260: 1052, 3109: 1053, 3113: 1054, 52265: 1055, 11307: 1056, 3132: 1057, 3133: 1058, 11324: 1059, 52288: 1060, 11330: 1061, 52290: 1062, 52295: 1063, 52296: 1064, 52297: 1065, 11339: 1066, 11341: 1067, 3151: 1068, 52305: 1069, 11356: 1070, 11357: 1071, 52318: 1072, 3170: 1073, 3172: 1074, 3175: 1075, 3176: 1076, 11369: 1077, 52330: 1078, 3179: 1079, 3181: 1080, 3182: 1081, 11376: 1082, 52348: 1083, 3200: 1084, 11394: 1085, 3207: 1086, 52360: 1087, 52362: 1088, 3212: 1089, 3216: 1090, 52374: 1091, 11423: 1092, 11425: 1093, 3235: 1094, 52390: 1095, 3239: 1096, 11431: 1097, 3242: 1098, 3246: 1099, 52399: 1100, 11440: 1101, 11443: 1102, 52405: 1103, 11452: 1104, 11458: 1105, 52418: 1106, 11460: 1107, 3275: 1108, 11469: 1109, 3277: 1110, 3279: 1111, 52436: 1112, 11478: 1113, 3288: 1114, 11481: 1115, 11484: 1116, 11485: 1117, 3294: 1118, 3296: 1119, 3301: 1120, 11497: 1121, 3307: 1122, 11504: 1123, 3314: 1124, 52466: 1125, 52472: 1126, 3321: 1127, 52477: 1128, 3326: 1129, 11521: 1130, 3330: 1131, 11523: 1132, 3331: 1133, 11525: 1134, 52485: 1135, 3335: 1136, 3337: 1137, 3338: 1138, 3347: 1139, 11539: 1140, 11545: 1141, 3355: 1142, 52512: 1143, 11554: 1144, 11555: 1145, 11561: 1146, 52524: 1147, 52527: 1148, 3380: 1149, 3382: 1150, 52536: 1151, 3384: 1152, 11580: 1153, 11585: 1154, 11592: 1155, 52553: 1156, 3402: 1157, 11596: 1158, 52559: 1159, 11600: 1160, 3409: 1161, 52562: 1162, 3416: 1163, 3421: 1164, 3424: 1165, 11621: 1166, 3430: 1167, 3431: 1168, 11625: 1169, 11627: 1170, 3435: 1171, 11633: 1172, 3442: 1173, 3446: 1174, 11642: 1175, 11643: 1176, 11652: 1177, 3471: 1178, 3474: 1179, 3476: 1180, 11670: 1181, 52632: 1182, 3484: 1183, 11681: 1184, 3491: 1185, 3493: 1186, 3500: 1187, 3501: 1188, 56056: 1189, 3504: 1190, 3506: 1191, 52663: 1192, 11708: 1193, 52670: 1194, 56060: 1195, 11718: 1196, 11719: 1197, 3529: 1198, 11722: 1199, 11721: 1200, 11727: 1201, 11741: 1202, 52702: 1203, 3553: 1204, 3556: 1205, 3557: 1206, 52712: 1207, 3565: 1208, 3567: 1209, 52720: 1210, 3578: 1211, 52731: 1212, 3588: 1213, 3590: 1214, 3592: 1215, 52748: 1216, 11789: 1217, 3598: 1218, 3600: 1219, 11797: 1220, 3607: 1221, 11800: 1222, 52763: 1223, 11811: 1224, 11813: 1225, 3622: 1226, 52775: 1227, 3625: 1228, 3627: 1229, 52781: 1230, 11823: 1231, 3635: 1232, 3638: 1233, 3641: 1234, 52793: 1235, 3645: 1236, 11839: 1237, 11844: 1238, 11846: 1239, 11857: 1240, 11860: 1241, 3672: 1242, 11865: 1243, 3675: 1244, 3679: 1245, 52834: 1246, 3683: 1247, 11879: 1248, 3690: 1249, 52844: 1250, 3693: 1251, 3694: 1252, 11886: 1253, 3698: 1254, 11891: 1255, 52850: 1256, 3702: 1257, 3707: 1258, 52861: 1259, 3709: 1260, 3713: 1261, 3720: 1262, 3721: 1263, 3722: 1264, 11919: 1265, 3728: 1266, 3735: 1267, 52891: 1268, 3741: 1269, 3749: 1270, 52903: 1271, 11944: 1272, 3753: 1273, 3755: 1274, 3756: 1275, 3759: 1276, 11951: 1277, 11955: 1278, 3765: 1279, 3772: 1280, 3773: 1281, 3775: 1282, 52928: 1283, 11976: 1284, 11984: 1285, 3804: 1286, 11999: 1287, 12002: 1288, 52965: 1289, 3815: 1290, 52970: 1291, 3819: 1292, 52979: 1293, 12020: 1294, 3830: 1295, 12029: 1296, 3840: 1297, 53003: 1298, 12050: 1299, 3862: 1300, 12055: 1301, 3864: 1302, 3863: 1303, 12059: 1304, 53020: 1305, 12066: 1306, 3875: 1307, 3878: 1308, 3879: 1309, 12076: 1310, 3886: 1311, 12079: 1312, 3894: 1313, 53052: 1314, 3906: 1315, 3911: 1316, 3916: 1317, 3917: 1318, 12112: 1319, 53072: 1320, 12113: 1321, 3923: 1322, 12116: 1323, 3925: 1324, 53078: 1325, 3929: 1326, 12121: 1327, 3937: 1328, 3938: 1329, 3941: 1330, 53098: 1331, 3947: 1332, 12140: 1333, 53101: 1334, 12150: 1335, 53112: 1336, 12154: 1337, 3967: 1338, 3971: 1339, 12163: 1340, 3972: 1341, 53127: 1342, 3976: 1343, 3980: 1344, 12174: 1345, 53135: 1346, 3988: 1347, 3989: 1348, 53144: 1349, 53146: 1350, 3995: 1351, 12189: 1352, 3999: 1353, 4003: 1354, 12199: 1355, 53161: 1356, 4010: 1357, 4015: 1358, 4016: 1359, 12207: 1360, 4018: 1361, 4019: 1362, 53172: 1363, 4022: 1364, 4023: 1365, 4025: 1366, 12223: 1367, 4033: 1368, 4037: 1369, 12237: 1370, 12238: 1371, 4048: 1372, 12240: 1373, 53202: 1374, 4051: 1375, 12244: 1376, 53201: 1377, 4054: 1378, 4058: 1379, 53211: 1380, 12256: 1381, 4067: 1382, 12260: 1383, 53221: 1384, 4071: 1385, 4080: 1386, 4083: 1387, 12276: 1388, 12277: 1389, 4092: 1390, 4094: 1391, 12289: 1392, 53251: 1393, 4105: 1394, 4107: 1395, 4112: 1396, 12305: 1397, 4114: 1398, 12304: 1399, 53268: 1400, 12307: 1401, 53289: 1402, 12332: 1403, 53294: 1404, 12334: 1405, 12343: 1406, 12354: 1407, 4164: 1408, 53318: 1409, 53320: 1410, 12361: 1411, 4169: 1412, 4172: 1413, 4173: 1414, 4176: 1415, 12371: 1416, 4180: 1417, 4182: 1418, 4184: 1419, 4187: 1420, 53341: 1421, 12383: 1422, 12385: 1423, 4194: 1424, 4200: 1425, 4206: 1426, 4209: 1427, 4210: 1428, 4213: 1429, 4215: 1430, 12409: 1431, 4221: 1432, 4224: 1433, 53379: 1434, 4233: 1435, 4237: 1436, 4244: 1437, 4249: 1438, 12441: 1439, 12450: 1440, 53411: 1441, 4260: 1442, 4259: 1443, 4258: 1444, 4264: 1445, 4266: 1446, 4271: 1447, 12467: 1448, 4277: 1449, 12474: 1450, 4288: 1451, 12483: 1452, 53444: 1453, 4294: 1454, 4296: 1455, 12493: 1456, 4316: 1457, 4321: 1458, 53477: 1459, 12518: 1460, 4325: 1461, 12520: 1462, 12522: 1463, 12525: 1464, 4338: 1465, 4339: 1466, 53495: 1467, 4344: 1468, 12537: 1469, 12538: 1470, 4349: 1471, 4352: 1472, 12548: 1473, 12549: 1474, 53509: 1475, 12554: 1476, 12558: 1477, 4366: 1478, 4371: 1479, 4373: 1480, 4376: 1481, 12572: 1482, 12573: 1483, 12575: 1484, 4386: 1485, 4387: 1486, 4389: 1487, 4393: 1488, 4394: 1489, 12588: 1490, 4397: 1491, 12594: 1492, 12599: 1493, 12607: 1494, 4416: 1495, 4421: 1496, 4422: 1497, 4426: 1498, 4427: 1499, 4433: 1500, 4435: 1501, 12629: 1502, 4441: 1503, 53597: 1504, 4447: 1505, 4453: 1506, 12646: 1507, 4455: 1508, 12650: 1509, 4463: 1510, 12656: 1511, 4466: 1512, 4469: 1513, 12664: 1514, 4473: 1515, 53628: 1516, 4485: 1517, 4486: 1518, 4489: 1519, 12681: 1520, 4490: 1521, 12686: 1522, 4495: 1523, 53648: 1524, 53651: 1525, 53652: 1526, 12695: 1527, 12699: 1528, 4511: 1529, 53665: 1530, 12706: 1531, 12721: 1532, 4532: 1533, 53686: 1534, 12727: 1535, 4536: 1536, 12729: 1537, 12733: 1538, 4545: 1539, 4550: 1540, 53704: 1541, 4556: 1542, 4557: 1543, 12769: 1544, 4579: 1545, 12774: 1546, 53734: 1547, 53736: 1548, 4585: 1549, 12777: 1550, 53741: 1551, 53742: 1552, 12783: 1553, 53748: 1554, 4600: 1555, 12792: 1556, 4615: 1557, 4616: 1558, 12809: 1559, 4621: 1560, 12815: 1561, 4623: 1562, 53780: 1563, 53782: 1564, 12823: 1565, 4633: 1566, 4637: 1567, 4640: 1568, 4653: 1569, 4655: 1570, 4656: 1571, 12850: 1572, 4662: 1573, 4663: 1574, 4666: 1575, 12858: 1576, 12861: 1577, 4676: 1578, 12869: 1579, 12868: 1580, 12873: 1581, 4682: 1582, 12876: 1583, 12880: 1584, 4688: 1585, 12888: 1586, 53850: 1587, 53858: 1588, 53860: 1589, 4710: 1590, 53865: 1591, 53869: 1592, 4721: 1593, 12917: 1594, 4727: 1595, 4730: 1596, 12923: 1597, 12925: 1598, 4734: 1599, 12927: 1600, 53889: 1601, 12936: 1602, 12937: 1603, 53898: 1604, 4752: 1605, 12945: 1606, 12951: 1607, 70301: 1608, 70302: 1609, 4767: 1610, 4769: 1611, 4778: 1612, 4782: 1613, 53935: 1614, 12980: 1615, 4789: 1616, 4796: 1617, 70334: 1618, 70335: 1619, 70336: 1620, 4802: 1621, 12997: 1622, 70342: 1623, 4809: 1624, 4811: 1625, 13004: 1626, 4813: 1627, 53966: 1628, 4812: 1629, 4817: 1630, 70355: 1631, 70356: 1632, 13012: 1633, 13014: 1634, 53979: 1635, 70363: 1636, 70365: 1637, 4838: 1638, 70375: 1639, 70376: 1640, 70378: 1641, 70379: 1642, 70380: 1643, 13036: 1644, 70381: 1645, 53996: 1646, 13040: 1647, 70385: 1648, 70389: 1649, 13046: 1650, 4858: 1651, 54011: 1652, 54013: 1653, 70398: 1654, 54015: 1655, 4862: 1656, 70406: 1657, 4871: 1658, 13068: 1659, 13070: 1660, 54030: 1661, 4880: 1662, 13079: 1663, 70424: 1664, 4889: 1665, 13081: 1666, 4890: 1667, 54047: 1668, 70431: 1669, 70433: 1670, 70437: 1671, 4902: 1672, 70441: 1673, 70442: 1674, 13099: 1675, 4908: 1676, 4905: 1677, 4910: 1678, 13097: 1679, 70445: 1680, 13105: 1681, 70450: 1682, 70451: 1683, 70452: 1684, 70453: 1685, 70454: 1686, 70449: 1687, 70448: 1688, 70457: 1689, 70458: 1690, 13110: 1691, 54077: 1692, 70462: 1693, 4925: 1694, 70464: 1695, 70465: 1696, 13122: 1697, 70467: 1698, 13121: 1699, 70466: 1700, 4933: 1701, 70463: 1702, 13135: 1703, 4944: 1704, 4949: 1705, 13142: 1706, 4951: 1707, 54111: 1708, 4960: 1709, 13154: 1710, 13162: 1711, 4975: 1712, 4979: 1713, 4984: 1714, 4986: 1715, 4989: 1716, 4990: 1717, 4991: 1718, 4992: 1719, 4995: 1720, 13188: 1721, 4999: 1722, 5001: 1723, 5002: 1724, 5005: 1725, 5008: 1726, 13202: 1727, 13206: 1728, 5030: 1729, 5032: 1730, 13229: 1731, 13230: 1732, 5042: 1733, 5048: 1734, 13243: 1735, 5052: 1736, 5054: 1737, 13247: 1738, 5056: 1739, 105212: 1740, 13256: 1741, 13257: 1742, 13264: 1743, 5074: 1744, 13267: 1745, 13269: 1746, 5077: 1747, 13271: 1748, 5080: 1749, 13277: 1750, 5099: 1751, 5101: 1752, 54256: 1753, 5104: 1754, 54259: 1755, 13301: 1756, 13302: 1757, 54261: 1758, 13304: 1759, 5129: 1760, 13325: 1761, 5141: 1762, 5144: 1763, 5151: 1764, 13346: 1765, 5158: 1766, 5162: 1767, 5169: 1768, 13375: 1769, 5185: 1770, 5190: 1771, 5191: 1772, 13383: 1773, 5193: 1774, 5199: 1775, 13394: 1776, 5202: 1777, 13397: 1778, 13401: 1779, 13409: 1780, 5227: 1781, 54385: 1782, 54386: 1783, 13427: 1784, 5238: 1785, 5255: 1786, 13452: 1787, 5264: 1788, 13457: 1789, 54418: 1790, 5268: 1791, 5270: 1792, 13464: 1793, 54425: 1794, 54432: 1795, 5281: 1796, 5286: 1797, 13482: 1798, 13483: 1799, 5294: 1800, 5299: 1801, 5301: 1802, 13498: 1803, 5307: 1804, 5311: 1805, 13513: 1806, 13515: 1807, 5324: 1808, 5325: 1809, 13516: 1810, 13517: 1811, 5331: 1812, 5337: 1813, 13531: 1814, 5339: 1815, 5340: 1816, 5343: 1817, 5344: 1818, 5345: 1819, 5352: 1820, 5357: 1821, 13550: 1822, 5359: 1823, 54511: 1824, 5362: 1825, 5367: 1826, 54520: 1827, 5368: 1828, 13559: 1829, 5369: 1830, 54525: 1831, 13568: 1832, 54533: 1833, 13579: 1834, 13585: 1835, 13586: 1836, 54549: 1837, 5397: 1838, 5406: 1839, 5407: 1840, 5416: 1841, 5417: 1842, 54571: 1843, 5420: 1844, 13615: 1845, 54575: 1846, 5431: 1847, 5432: 1848, 5433: 1849, 5436: 1850, 5437: 1851, 5443: 1852, 13640: 1853, 5450: 1854, 13643: 1855, 54609: 1856, 13651: 1857, 5465: 1858, 5478: 1859, 5479: 1860, 5489: 1861, 13684: 1862, 5499: 1863, 5501: 1864, 13698: 1865, 13699: 1866, 5513: 1867, 13706: 1868, 5521: 1869, 5524: 1870, 13718: 1871, 5530: 1872, 5535: 1873, 13727: 1874, 5538: 1875, 13731: 1876, 13734: 1877, 5544: 1878, 13739: 1879, 13740: 1880, 5550: 1881, 5551: 1882, 5552: 1883, 13745: 1884, 13746: 1885, 13748: 1886, 13758: 1887, 5571: 1888, 13767: 1889, 13768: 1890, 5586: 1891, 5589: 1892, 5590: 1893, 13784: 1894, 5594: 1895, 5597: 1896, 5599: 1897, 5603: 1898, 5608: 1899, 13802: 1900, 5617: 1901, 5624: 1902, 5626: 1903, 5630: 1904, 5638: 1905, 5643: 1906, 5647: 1907, 5649: 1908, 13842: 1909, 13845: 1910, 13846: 1911, 5654: 1912, 5659: 1913, 5670: 1914, 5671: 1915, 5672: 1916, 5674: 1917, 5678: 1918, 54831: 1919, 13872: 1920, 5682: 1921, 54838: 1922, 54839: 1923, 13884: 1924, 54845: 1925, 13885: 1926, 5695: 1927, 13891: 1928, 54852: 1929, 5713: 1930, 5716: 1931, 54870: 1932, 5719: 1933, 5720: 1934, 5726: 1935, 5728: 1936, 5730: 1937, 5731: 1938, 13923: 1939, 5733: 1940, 13928: 1941, 54890: 1942, 5743: 1943, 5744: 1944, 13937: 1945, 5749: 1946, 54912: 1947, 13959: 1948, 5767: 1949, 5769: 1950, 54925: 1951, 5774: 1952, 5776: 1953, 5784: 1954, 5794: 1955, 5806: 1956, 5809: 1957, 5812: 1958, 5813: 1959, 14008: 1960, 5817: 1961, 5823: 1962, 54976: 1963, 5825: 1964, 5826: 1965, 54981: 1966, 5829: 1967, 5834: 1968, 5848: 1969, 55000: 1970, 5853: 1971, 5862: 1972, 5866: 1973, 5869: 1974, 5877: 1975, 55030: 1976, 5879: 1977, 55048: 1978, 5897: 1979, 5899: 1980, 5911: 1981, 5912: 1982, 5921: 1983, 55076: 1984, 5952: 1985, 5959: 1986, 5963: 1987, 5965: 1988, 5972: 1989, 5984: 1990, 5985: 1991, 5998: 1992, 6002: 1993, 6004: 1994, 6010: 1995, 6011: 1996, 6018: 1997, 6019: 1998, 6028: 1999, 6033: 2000, 6061: 2001, 6066: 2002, 6067: 2003, 6068: 2004, 6073: 2005, 6081: 2006, 6088: 2007, 6097: 2008, 6110: 2009, 6116: 2010, 6121: 2011, 6122: 2012, 6137: 2013, 6140: 2014, 6144: 2015, 6149: 2016, 6156: 2017, 6160: 2018, 6163: 2019, 6167: 2020, 6173: 2021, 6177: 2022, 6182: 2023, 6186: 2024, 6187: 2025, 6193: 2026, 6194: 2027, 6206: 2028, 6221: 2029, 6229: 2030, 6232: 2031, 6233: 2032, 6234: 2033, 6236: 2034, 6250: 2035, 6253: 2036, 6254: 2037, 6258: 2038, 6262: 2039, 6269: 2040, 6283: 2041, 6284: 2042, 6288: 2043, 6289: 2044, 6290: 2045, 6296: 2046, 6299: 2047, 6303: 2048, 6308: 2049, 6312: 2050, 6314: 2051, 6315: 2052, 6349: 2053, 6350: 2054, 6357: 2055, 6361: 2056, 6367: 2057, 6373: 2058, 6380: 2059, 6386: 2060, 55538: 2061, 55551: 2062, 6400: 2063, 6399: 2064, 6408: 2065, 6409: 2066, 6410: 2067, 6415: 2068, 6417: 2069, 6420: 2070, 6432: 2071, 55584: 2072, 6435: 2073, 6440: 2074, 6442: 2075, 6454: 2076, 6458: 2077, 6460: 2078, 6466: 2079, 6468: 2080, 6484: 2081, 6487: 2082, 6491: 2083, 6499: 2084, 6501: 2085, 6504: 2086, 6506: 2087, 55677: 2088, 6528: 2089, 55682: 2090, 6532: 2091, 55688: 2092, 6540: 2093, 55698: 2094, 6558: 2095, 55714: 2096, 55715: 2097, 6567: 2098, 6570: 2099, 55723: 2100, 55731: 2101, 6590: 2102, 6594: 2103, 6598: 2104, 6601: 2105, 55756: 2106, 55757: 2107, 6606: 2108, 6607: 2109, 55765: 2110, 55778: 2111, 55779: 2112, 6636: 2113, 6637: 2114, 6640: 2115, 6642: 2116, 6655: 2117, 6662: 2118, 6664: 2119, 6669: 2120, 6676: 2121, 6683: 2122, 6686: 2123, 6690: 2124, 6691: 2125, 6694: 2126, 6715: 2127, 6716: 2128, 6719: 2129, 6721: 2130, 6723: 2131, 6737: 2132, 6739: 2133, 55897: 2134, 6753: 2135, 6757: 2136, 6763: 2137, 55918: 2138, 6782: 2139, 6784: 2140, 6801: 2141, 6804: 2142, 6808: 2143, 6809: 2144, 55963: 2145, 6815: 2146, 6826: 2147, 6840: 2148, 6842: 2149, 6846: 2150, 6847: 2151, 6857: 2152, 6864: 2153, 6872: 2154, 56035: 2155, 6885: 2156, 6886: 2157, 6890: 2158, 6895: 2159, 6901: 2160, 105208: 2161, 105209: 2162, 105210: 2163, 6906: 2164, 105211: 2165, 105213: 2166, 105214: 2167, 105215: 2168, 105216: 2169, 105217: 2170, 56063: 2171, 105219: 2172, 6916: 2173, 105218: 2174, 6914: 2175, 56078: 2176, 6930: 2177, 56085: 2178, 6933: 2179, 6937: 2180, 6940: 2181, 6943: 2182, 6947: 2183, 56107: 2184, 6959: 2185, 6962: 2186, 6973: 2187, 6977: 2188, 6980: 2189, 6984: 2190, 56137: 2191, 6986: 2192, 7008: 2193, 7018: 2194, 7019: 2195, 7022: 2196, 7024: 2197, 7027: 2198, 7028: 2199, 7031: 2200, 7032: 2201, 7041: 2202, 56203: 2203, 56222: 2204, 7070: 2205, 7073: 2206, 7088: 2207, 7095: 2208, 56251: 2209, 56257: 2210, 7106: 2211, 7107: 2212, 7110: 2213, 7111: 2214, 7117: 2215, 7118: 2216, 56271: 2217, 7123: 2218, 7124: 2219, 7140: 2220, 7145: 2221, 7150: 2222, 7153: 2223, 7155: 2224, 56310: 2225, 7161: 2226, 7163: 2227, 7166: 2228, 7172: 2229, 7177: 2230, 56332: 2231, 7183: 2232, 7198: 2233, 7201: 2234, 7212: 2235, 56375: 2236, 56379: 2237, 7233: 2238, 7240: 2239, 7249: 2240, 7277: 2241, 7280: 2242, 7285: 2243, 7286: 2244, 7287: 2245, 7291: 2246, 7292: 2247, 7299: 2248, 7302: 2249, 7308: 2250, 7312: 2251, 7317: 2252, 7320: 2253, 7321: 2254, 7340: 2255, 7344: 2256, 7347: 2257, 7348: 2258, 7349: 2259, 7350: 2260, 7356: 2261, 7371: 2262, 7379: 2263, 7380: 2264, 7389: 2265, 7390: 2266, 7403: 2267, 7404: 2268, 7413: 2269, 56566: 2270, 7422: 2271, 7427: 2272, 56584: 2273, 7437: 2274, 7445: 2275, 7452: 2276, 7453: 2277, 7456: 2278, 7461: 2279, 7462: 2280, 7464: 2281, 7465: 2282, 7467: 2283, 7470: 2284, 7487: 2285, 7488: 2286, 7491: 2287, 7501: 2288, 7509: 2289, 56663: 2290, 56664: 2291, 7516: 2292, 7517: 2293, 7526: 2294, 7527: 2295, 7534: 2296, 7546: 2297, 7566: 2298, 7568: 2299, 7570: 2300, 7575: 2301, 7591: 2302, 7597: 2303, 7607: 2304, 7618: 2305, 7621: 2306, 7626: 2307, 7640: 2308, 7647: 2309, 7661: 2310, 7667: 2311, 7669: 2312, 7670: 2313, 7672: 2314, 7682: 2315, 7685: 2316, 7688: 2317, 7694: 2318, 7696: 2319, 7713: 2320, 7715: 2321, 7724: 2322, 7729: 2323, 7733: 2324, 7742: 2325, 7743: 2326, 7744: 2327, 7752: 2328, 7775: 2329, 56935: 2330, 7784: 2331, 7787: 2332, 7792: 2333, 7798: 2334, 56950: 2335, 7804: 2336, 56964: 2337, 7814: 2338, 7817: 2339, 7818: 2340, 7827: 2341, 7848: 2342, 57001: 2343, 7851: 2344, 7854: 2345, 7862: 2346, 57015: 2347, 7864: 2348, 7866: 2349, 7885: 2350, 7893: 2351, 7896: 2352, 7899: 2353, 7902: 2354, 57058: 2355, 7910: 2356, 7915: 2357, 7922: 2358, 57078: 2359, 57081: 2360, 57085: 2361, 7937: 2362, 7938: 2363, 57090: 2364, 7943: 2365, 7951: 2366, 7955: 2367, 57119: 2368, 7978: 2369, 57134: 2370, 7983: 2371, 7997: 2372, 8010: 2373, 8014: 2374, 8018: 2375, 8030: 2376, 57182: 2377, 8036: 2378, 8040: 2379, 8044: 2380, 57199: 2381, 8060: 2382, 8068: 2383, 8071: 2384, 8081: 2385, 8083: 2386, 57242: 2387, 8093: 2388, 8113: 2389, 8114: 2390, 57268: 2391, 8117: 2392, 8125: 2393, 8133: 2394, 57293: 2395, 8148: 2396, 8154: 2397, 8156: 2398, 8165: 2399, 8167: 2400, 8178: 2401, 8183: 2402, 8190: 2403, 8191: 2404}\n"
     ]
    }
   ],
   "source": [
    "## To assign each unqiue true gene id to a class label\n",
    "\n",
    "import torch\n",
    "\n",
    "all_true_gene_ids = []\n",
    "\n",
    "for patient in train_pg_subgraph:\n",
    "    all_true_gene_ids.extend(patient.true_gene_ids)\n",
    "\n",
    "for patient in val_pg_subgraph:\n",
    "    all_true_gene_ids.extend(patient.true_gene_ids)\n",
    "    \n",
    "for patient in test_pg_subgraph:\n",
    "    all_true_gene_ids.extend(patient.true_gene_ids)\n",
    "\n",
    "## Get the unique true gene ids\n",
    "unique_true_gene_ids = set(all_true_gene_ids)\n",
    "print(\"the number of unique true gene ids is:\",len(unique_true_gene_ids))\n",
    "\n",
    "## Mapping all unique true gene ids to a index from 0 to the number of unique true gene ids\n",
    "gene_id_mapping = {gene_id: idx for idx, gene_id in enumerate(unique_true_gene_ids)}\n",
    "print(gene_id_mapping)\n",
    "\n",
    "## Add the true gene ids back to the graph\n",
    "for patient in train_pg_subgraph:\n",
    "    patient.y = torch.tensor([gene_id_mapping[gene_id] for gene_id in patient.true_gene_ids], dtype=torch.long)\n",
    "\n",
    "for patient in val_pg_subgraph:\n",
    "    patient.y = torch.tensor([gene_id_mapping[gene_id] for gene_id in patient.true_gene_ids], dtype=torch.long)\n",
    "\n",
    "for patient in test_pg_subgraph:\n",
    "    patient.y = torch.tensor([gene_id_mapping[gene_id] for gene_id in patient.true_gene_ids], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract task relevant data, only patients features, true gene id, feature connections for graph convolution\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def preprocess_graph_data(dataset):\n",
    "    processed_graphs = []\n",
    "   \n",
    "    for data in dataset:\n",
    "        \n",
    "        new_data = Data(\n",
    "            edge_index=data.edge_index,\n",
    "            y=data.y,\n",
    "            x=data.x,\n",
    "            original_ids = data.original_ids,\n",
    "            edge_attr=data.edge_attr\n",
    "        )\n",
    "        processed_graphs.append(new_data)\n",
    "    \n",
    "    return processed_graphs\n",
    "\n",
    "train_data = preprocess_graph_data(train_pg_subgraph)\n",
    "val_data = preprocess_graph_data(val_pg_subgraph)\n",
    "test_data = preprocess_graph_data(test_pg_subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define collate function for handling batched data\n",
    "def optimized_collate_fn(batch):\n",
    "\n",
    "    batch_size = len(batch)\n",
    "    cumsum_nodes = 0\n",
    "    \n",
    "    # Adjust edge indices to account for the node offset in each batch\n",
    "    adjusted_edge_indices = []\n",
    "    for data in batch:\n",
    "        edge_index = data.edge_index + cumsum_nodes\n",
    "        adjusted_edge_indices.append(edge_index)\n",
    "        cumsum_nodes += data.num_nodes\n",
    "\n",
    "    # Concatenate with adjusted indices\n",
    "    x = torch.cat([data.x for data in batch], dim=0)\n",
    "    y = torch.cat([data.y for data in batch], dim=0)\n",
    "    edge_index = torch.cat(adjusted_edge_indices, dim=1)\n",
    "    edge_attr = torch.cat([data.edge_attr for data in batch], dim=0) if batch[0].edge_attr is not None else None\n",
    "    batch_tensor = torch.cat([torch.full((data.num_nodes,), i, dtype=torch.long) for i, data in enumerate(batch)])\n",
    "    \n",
    "    ## Additional attributes\n",
    "    original_ids = torch.cat([torch.tensor(data.original_ids, dtype=torch.long) if isinstance(data.original_ids, list) else data.original_ids for data in batch if data.original_ids is not None])\n",
    "    \n",
    "    return Data(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        batch=batch_tensor,\n",
    "        original_ids=original_ids,\n",
    "        batch_size=batch_size,\n",
    "      \n",
    "    )\n",
    "## torch.dataloader doesn't consider custom data types\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True, collate_fn=optimized_collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=256, collate_fn=optimized_collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=128, collate_fn=optimized_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the GlobalNodeEmbedding to turn the discrete node ids into embeddings vectors \n",
    "import torch.nn as nn\n",
    "\n",
    "class GlobalNodeEmbedding(nn.Module):\n",
    "    def __init__(self, num_global_nodes, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_global_nodes, embedding_dim)\n",
    "   \n",
    "    def forward(self, node_ids):\n",
    "        if isinstance(node_ids, list):\n",
    "            node_ids = torch.tensor(node_ids, dtype=torch.long)\n",
    "        node_ids = node_ids.view(-1)  # Flatten any multi-dimensional input\n",
    "\n",
    "        embeddings = self.embedding(node_ids)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the custom SoftHistogram\n",
    "\n",
    "class SoftHistogram(nn.Module):\n",
    "    def __init__(self, bins, min, max, sigma):\n",
    "        super().__init__()\n",
    "        self.bins = bins\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        self.sigma = sigma\n",
    "        self.delta = (max - min) / bins\n",
    "        self.centers = torch.linspace(min + self.delta/2, max - self.delta/2, bins)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1)\n",
    "        centers = self.centers.to(x.device).view(1, -1)\n",
    "        # Compute Gaussian smoothed histogram\n",
    "        x = torch.exp(-(x - centers)**2 / (2 * self.sigma**2))\n",
    "        # Normalize each data point's contribution\n",
    "        x = x / x.sum(dim=1, keepdim=True)\n",
    "        # Sum over all data points\n",
    "        histogram = x.sum(dim=0)\n",
    "        return histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main GiG Framework\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GraphConv, SAGEConv, GIN, \n",
    "    global_mean_pool, global_add_pool\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "## Node-Level Module (F1)\n",
    "class F1NodeLevelModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, conv_type='GCN',\n",
    "                 dropout=0.5, pooling=\"mean\", num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Choose GNN Layer Type\n",
    "        if conv_type == \"GCN\":\n",
    "            Conv = GCNConv\n",
    "        elif conv_type == \"Graph\":\n",
    "            Conv = GraphConv\n",
    "        elif conv_type == \"SAGE\":\n",
    "            Conv = SAGEConv\n",
    "        elif conv_type == \"GIN\":\n",
    "            Conv = lambda in_dim, out_dim: GIN(\n",
    "                nn=nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(out_dim, out_dim)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown conv_type: {conv_type}\")\n",
    "\n",
    "        # Define GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        # First Layer\n",
    "        self.convs.append(Conv(input_dim, hidden_dim))\n",
    "        self.bns.append(nn.LayerNorm(hidden_dim))  # Using LayerNorm for stability\n",
    "\n",
    "        # Hidden Layers (Adding Residual Connections)\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(Conv(hidden_dim, hidden_dim))\n",
    "            self.bns.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        # Last Layer\n",
    "        self.convs.append(Conv(hidden_dim, embedding_dim))\n",
    "        self.bns.append(nn.LayerNorm(embedding_dim))  # Ensure correct dimension\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.pooling = global_mean_pool if pooling == \"mean\" else global_add_pool\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for conv, bn in zip(self.convs[:-1], self.bns[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        x = self.bns[-1](x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        node_embeddings = x\n",
    "        graph_embeddings = self.pooling(node_embeddings, batch)\n",
    "\n",
    "        return node_embeddings, graph_embeddings\n",
    "\n",
    "\n",
    "## Population-Level Module (F2)\n",
    "class F2PopulationLevelGraph(nn.Module):\n",
    "    def __init__(self, embedding_dim, latent_dim, temperature=0.5, threshold=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simplify the transformation network\n",
    "        self.latent_transform = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Use direct parameters like the tutor\n",
    "        self.temp = nn.Parameter(torch.tensor(temperature, dtype=torch.float32))\n",
    "        self.theta = nn.Parameter(torch.tensor(threshold, dtype=torch.float32))\n",
    "        self.mu = nn.Parameter(torch.tensor(2.0, dtype=torch.float32))\n",
    "        self.sigma = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, graph_embeddings):\n",
    "        # Transform to latent space with the neural network\n",
    "        latent_space = self.latent_transform(graph_embeddings)\n",
    "        \n",
    "        # Compute the pairwise differences exactly as tutor does\n",
    "        diff = latent_space.unsqueeze(1) - latent_space.unsqueeze(0)\n",
    "        # Compute the squared norm\n",
    "        diff = torch.pow(diff, 2).sum(2)\n",
    "        mask_diff = diff != 0.0\n",
    "        dist = - torch.sqrt(diff + torch.finfo(torch.float32).eps)\n",
    "        dist = dist * mask_diff\n",
    "        \n",
    "        # Apply temperature and threshold like tutor\n",
    "        prob_matrix = self.temp * dist + self.theta\n",
    "        \n",
    "        # Add eye and sigmoid\n",
    "        adj = prob_matrix + torch.eye(prob_matrix.shape[0]).to(prob_matrix.device)\n",
    "        adjacency_matrix = torch.sigmoid(adj)\n",
    "        \n",
    "        # Extract edges for the GNN\n",
    "        edge_indices = torch.nonzero(adjacency_matrix > 0.1, as_tuple=False)\n",
    "        edge_index = edge_indices.t()\n",
    "        edge_weight = adjacency_matrix[edge_indices[:, 0], edge_indices[:, 1]]\n",
    "        \n",
    "        # Calculate KL loss using tutor's approach\n",
    "        n_nodes = adjacency_matrix.shape[0]\n",
    "        softhist = SoftHistogram(bins=n_nodes, min=0.5, max=n_nodes + 0.5, sigma=0.6)\n",
    "        kl_loss = self._compute_kl_loss(adjacency_matrix, n_nodes, softhist)\n",
    "        \n",
    "        return adjacency_matrix, edge_index, edge_weight, kl_loss\n",
    "    \n",
    "    def _compute_kl_loss(self, adj, batch_size, softhist):\n",
    "        # Create binary adjacency matrix\n",
    "        binarized_adj = torch.zeros(adj.shape).to(adj.device)\n",
    "        binarized_adj[adj > 0.5] = 1\n",
    "        \n",
    "        # Get distribution and degrees\n",
    "        dist, deg = self._compute_distr(adj * binarized_adj, softhist)\n",
    "        \n",
    "        # Get target distribution\n",
    "        target_dist = self._compute_target_distribution(batch_size)\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        kl_loss = self._kl_div(dist, target_dist)\n",
    "        return kl_loss\n",
    "    \n",
    "    def _kl_div(self, p, q):\n",
    "        return torch.sum(p * torch.log(p / (q + 1e-8) + 1e-8))\n",
    "    \n",
    "    def _compute_distr(self, adj, softhist):\n",
    "        deg = adj.sum(-1)\n",
    "        distr = softhist(deg)\n",
    "        return distr / torch.sum(distr), deg\n",
    "    \n",
    "    def _compute_target_distribution(self, batch_size):\n",
    "        \"\"\"Compute Gaussian target distribution like tutor\"\"\"\n",
    "        device = self.mu.device\n",
    "        target_distribution = torch.zeros(batch_size).to(device)\n",
    "        \n",
    "        # Use all bins\n",
    "        indices = torch.arange(batch_size, device=device)\n",
    "        \n",
    "        # Create Gaussian distribution \n",
    "        target_distribution = torch.exp(\n",
    "            -((self.mu - indices) ** 2) / (self.sigma ** 2)\n",
    "        )\n",
    "        \n",
    "        # Normalize\n",
    "        return target_distribution / target_distribution.sum()\n",
    "\n",
    "## Classifier Module (F3)\n",
    "class F3Classifier(nn.Module):\n",
    "    def __init__(self, input_dim_h, gnn_hidden_dim, num_classes, conv_type=\"GCN\", gnn_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim_h = input_dim_h\n",
    "        self.gnn_hidden_dim = gnn_hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Choose GNN Layer\n",
    "        if conv_type == \"GCN\":\n",
    "            Conv = GCNConv\n",
    "        else:\n",
    "            Conv = GraphConv\n",
    "        \n",
    "        # Complex input transformation with multiple pathways\n",
    "        self.input_transform = nn.Sequential(\n",
    "            # Branch 1: Direct mapping\n",
    "            nn.Linear(input_dim_h, gnn_hidden_dim),\n",
    "            nn.BatchNorm1d(gnn_hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.deep_transform = nn.Sequential(\n",
    "            nn.Linear(input_dim_h, gnn_hidden_dim),\n",
    "            nn.LayerNorm(gnn_hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout/2),\n",
    "            nn.Linear(gnn_hidden_dim, gnn_hidden_dim),\n",
    "            nn.LayerNorm(gnn_hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout/2)\n",
    "        )\n",
    "       \n",
    "        # Simpler GNN stack\n",
    "        self.gnn_layers = nn.ModuleList([Conv(gnn_hidden_dim, gnn_hidden_dim) for _ in range(gnn_layers)])\n",
    "        \n",
    "        # In F3Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(gnn_hidden_dim, gnn_hidden_dim*2),\n",
    "            nn.BatchNorm1d(gnn_hidden_dim*2),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(gnn_hidden_dim*2, gnn_hidden_dim),\n",
    "            nn.BatchNorm1d(gnn_hidden_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.1), \n",
    "            nn.Linear(gnn_hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, h, edge_index, batch, Ap=None, edge_weight=None, gene_ids=None):\n",
    "        # First transform input\n",
    "        h = F.relu(self.input_transform(h))\n",
    "        \n",
    "        # Process through GNN layers\n",
    "        for gnn in self.gnn_layers:\n",
    "            if edge_weight is not None and isinstance(gnn, (GCNConv, GraphConv)):\n",
    "                h = gnn(h, edge_index, edge_weight)\n",
    "            else:\n",
    "                h = gnn(h, edge_index)\n",
    "            h = F.relu(h)\n",
    "        \n",
    "        # Simple pooling\n",
    "        graph_embeddings = global_mean_pool(h, batch)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(graph_embeddings)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GiG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_level_module = F1NodeLevelModule(\n",
    "            input_dim=config[\"input_dim\"],\n",
    "            hidden_dim=config[\"hidden_dim\"],\n",
    "            embedding_dim=config[\"embedding_dim\"],\n",
    "            conv_type=config[\"conv_type\"],\n",
    "            dropout=config[\"dropout\"]\n",
    "        )\n",
    "        \n",
    "        self.population_level_module = F2PopulationLevelGraph(\n",
    "            embedding_dim=config[\"embedding_dim\"],\n",
    "            latent_dim=config[\"latent_dim\"]\n",
    "        )\n",
    "        \n",
    "        self.classifier = F3Classifier(\n",
    "            input_dim_h=config[\"embedding_dim\"],  # Use embedding_dim from F1\n",
    "            gnn_hidden_dim=config[\"gnn_hidden_dim\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            conv_type=config[\"conv_type\"],\n",
    "            gnn_layers=config[\"gnn_layers\"],\n",
    "            dropout=config[\"dropout\"]\n",
    "        )\n",
    "    def forward(self, data):\n",
    "        # Process inputs through node-level module\n",
    "        node_embeddings, graph_embeddings = self.node_level_module(data)\n",
    "        \n",
    "        # Process through population-level module\n",
    "        adjacency_matrix, edge_index, edge_weight, kl_loss = self.population_level_module(graph_embeddings)\n",
    "        \n",
    "        # Classifier takes embeddings and edge structure\n",
    "        logits = self.classifier(\n",
    "            node_embeddings,\n",
    "            edge_index, \n",
    "            data.batch,\n",
    "            adjacency_matrix,\n",
    "            edge_weight\n",
    "        )\n",
    "        \n",
    "        return logits, adjacency_matrix, kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Define Loss Dictionary\n",
    "losses = nn.ModuleDict({\n",
    "    'BCEWithLogitsLoss': nn.BCEWithLogitsLoss(),\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'MultiTaskBCE': nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([10]))\n",
    "})\n",
    "\n",
    "class GiGTrainer(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.save_hyperparameters(config)\n",
    "        self.automatic_optimization = False  # Manual optimization\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = GiG(config)\n",
    "\n",
    "        # Check and define GlobalNodeEmbedding properly\n",
    "        if \"GlobalNodeEmbedding\" in globals():\n",
    "            self.global_node_embedding = GlobalNodeEmbedding(\n",
    "                num_global_nodes=105220, embedding_dim=config[\"input_dim\"]\n",
    "            )\n",
    "        else:\n",
    "            self.global_node_embedding = nn.Embedding(\n",
    "                num_embeddings=105220, embedding_dim=config[\"input_dim\"]\n",
    "            )\n",
    "\n",
    "        # Set loss function\n",
    "        self.initial_loss = losses[config[\"loss\"]]\n",
    "        self.alpha = config[\"alpha\"]\n",
    "\n",
    "        # Store embeddings for debugging\n",
    "        self.node_embeddings = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "\n",
    "    def _shared_step(self, data, addition):\n",
    "        \"\"\"Common logic for train, validation, and test steps with NaN protection.\"\"\"\n",
    "        # Ensure input embeddings are used correctly\n",
    "        data.x = self.global_node_embedding(data.original_ids.long().to(self.device))\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, adj_matrix, kl_loss = self.model(data)\n",
    "        \n",
    "        # Prepare labels\n",
    "        labels = data.y.view(-1).long().to(self.device)\n",
    "        \n",
    "        # Compute classification loss\n",
    "        classification_loss = self.initial_loss(logits, data.y.view(-1).long())\n",
    "\n",
    "        \n",
    "        # Handle NaN classification loss\n",
    "        if torch.isnan(classification_loss):\n",
    "            print(f\"Warning: NaN in classification loss detected in {addition} step\")\n",
    "            classification_loss = torch.tensor(5.0, device=self.device)  # Reasonable default\n",
    "        \n",
    "        # Apply very small KL loss weight initially\n",
    "        kl_weight = self.alpha \n",
    "    \n",
    "        # Total loss\n",
    "        total_loss = classification_loss + kl_weight * kl_loss\n",
    "        \n",
    "        # Compute metrics safely\n",
    "        try:\n",
    "            acc = torchmetrics.functional.accuracy(\n",
    "                logits.argmax(dim=-1), labels, task=\"multiclass\", num_classes=logits.shape[1]\n",
    "            )\n",
    "            f1 = torchmetrics.functional.f1_score(\n",
    "                logits.argmax(dim=-1), labels, task=\"multiclass\", num_classes=logits.shape[1]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing metrics: {e}\")\n",
    "            acc = torch.tensor(0.0)\n",
    "            f1 = torch.tensor(0.0)\n",
    "                \n",
    "        # Log metrics\n",
    "        metrics = {\n",
    "            f\"{addition}_acc\": acc,\n",
    "            f\"{addition}_f1\": f1,\n",
    "            f\"{addition}_loss\": total_loss,\n",
    "            f\"{addition}_classification_loss\": classification_loss,\n",
    "            f\"{addition}_kl_loss\": kl_loss,\n",
    "        }\n",
    "        \n",
    "        return metrics, total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step with gradient clipping and NaN detection.\"\"\"\n",
    "        # Get optimizers\n",
    "        main_optimizer, lgl_optimizer = self.optimizers()\n",
    "        \n",
    "        # Zero gradients\n",
    "        main_optimizer.zero_grad()\n",
    "        lgl_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and loss calculation\n",
    "        metrics, loss = self._shared_step(batch, \"train\")\n",
    "    \n",
    "        # Check if loss is valid\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"Warning: Non-finite loss detected: {loss}\")\n",
    "            # Return a default loss to continue training\n",
    "            placeholder_loss = torch.tensor(5.0, device=self.device, requires_grad=True)\n",
    "            # Use log_dict with on_step=True, on_epoch=True\n",
    "            self.log_dict(metrics, prog_bar=True, batch_size=len(batch.y), \n",
    "                        on_step=False, on_epoch=True)\n",
    "            return placeholder_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        self.manual_backward(loss)\n",
    "        \n",
    "        # Check for NaN/Inf gradients before optimizer step\n",
    "        valid_gradients = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if not torch.isfinite(param.grad).all():\n",
    "                    print(f\"Warning: Non-finite gradients in {name}\")\n",
    "                    valid_gradients = False\n",
    "                    param.grad = torch.zeros_like(param.grad)  # Reset problematic gradients\n",
    "        \n",
    "        # Clip gradients to prevent explosion\n",
    "        if valid_gradients:\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        main_optimizer.step()\n",
    "        lgl_optimizer.step()\n",
    "        \n",
    "        # Log metrics - correctly specify on_step and on_epoch\n",
    "        self.log_dict(metrics, prog_bar=True, batch_size=len(batch.y),\n",
    "                    on_step=False, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        metrics, _ = self._shared_step(batch, \"val\")\n",
    "        \n",
    "        with torch.no_grad():  # Ensure we don't compute gradients\n",
    "            node_embeddings, _ = self.model.node_level_module(batch)\n",
    "            node_embeddings = node_embeddings.detach().cpu()\n",
    "            \n",
    "            # Store embeddings for later analysis\n",
    "            if self.node_embeddings is None:\n",
    "                self.node_embeddings = node_embeddings\n",
    "            else:\n",
    "                self.node_embeddings = torch.cat([self.node_embeddings, node_embeddings], dim=0)\n",
    "    \n",
    "        # Add on_step and on_epoch parameters\n",
    "        self.log_dict(metrics, prog_bar=True, batch_size=len(batch.y),\n",
    "                    on_step=False, on_epoch=True)\n",
    "        return metrics\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        metrics, _ = self._shared_step(batch, \"test\")\n",
    "        \n",
    "        # Add on_step and on_epoch parameters\n",
    "        self.log_dict(metrics, prog_bar=True, batch_size=len(batch.y),\n",
    "                    on_step=False, on_epoch=True)\n",
    "        return metrics\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Set up optimizers and learning rate schedulers.\"\"\"\n",
    "        # Main parameters excluding population graph learnable scalars\n",
    "        main_params = [\n",
    "            param for name_, param in self.model.population_level_module.named_parameters()\n",
    "            if name_ not in [\"log_temperature\", \"log_threshold\", \"mu\", \"sigma\"]\n",
    "        ]\n",
    "        main_params.extend(self.model.node_level_module.parameters())\n",
    "        main_params.extend(self.model.classifier.parameters())\n",
    "\n",
    "        # Define optimizers with weight decay\n",
    "        main_optimizer = torch.optim.Adam(\n",
    "            main_params, \n",
    "            lr=self.config[\"lr\"],\n",
    "            weight_decay=self.config.get(\"weight_decay\", 1e-5)\n",
    "        )\n",
    "        \n",
    "        # Updated parameter names for log-space parameters\n",
    "        # In configure_optimizers method\n",
    "        lgl_optimizer = torch.optim.Adam([\n",
    "            self.model.population_level_module.temp,      # Changed from log_temperature\n",
    "            self.model.population_level_module.theta,     # Changed from log_threshold\n",
    "            self.model.population_level_module.mu,\n",
    "            self.model.population_level_module.sigma\n",
    "        ], lr=self.config[\"lr_theta_temp\"])\n",
    "\n",
    "        # Define learning rate scheduler\n",
    "        if self.config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler_dict = {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    main_optimizer, mode=\"min\", patience=5,\n",
    "                    threshold=0.001, verbose=True\n",
    "                ),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        else:\n",
    "            scheduler_dict = {\n",
    "                \"scheduler\": CosineAnnealingLR(main_optimizer, T_max=10),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "\n",
    "        return [main_optimizer, lgl_optimizer], [scheduler_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                | Params | Mode \n",
      "----------------------------------------------------------------------\n",
      "0 | model                 | GiG                 | 585 K  | train\n",
      "1 | global_node_embedding | GlobalNodeEmbedding | 6.7 M  | train\n",
      "2 | initial_loss          | CrossEntropyLoss    | 0      | train\n",
      "----------------------------------------------------------------------\n",
      "7.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 M     Total params\n",
      "29.280    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a9463037549e0a5e977d722e95398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a11d7e9b4f4772a48524834d56ac78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398c5f9b227841c59ae5113b408e75c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1267047f88b4ad2bae0117f225eeb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f679a84d988e409d82c4dc583c27a58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor\n",
    ")\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='checkpoints',\n",
    "        filename='gig-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "    ),\n",
    "    # EarlyStopping(\n",
    "    #     monitor='val_loss',\n",
    "    #     min_delta=0.01,\n",
    "    #     patience=10,\n",
    "    #     mode='min',\n",
    "    #     verbose=True\n",
    "    # ),\n",
    "    LearningRateMonitor(logging_interval='epoch')\n",
    "]\n",
    "\n",
    "# Setup logger\n",
    "wandb_logger = WandbLogger(project=\"gig-model\")\n",
    "\n",
    "config = {\n",
    "    \"input_dim\": 64,\n",
    "    \"hidden_dim\": 128,     \n",
    "    \"embedding_dim\": 128,   \n",
    "    \"latent_dim\": 128,      \n",
    "    \"gnn_hidden_dim\":128 ,  \n",
    "    \"num_classes\": len(unique_true_gene_ids),\n",
    "    \n",
    "    \"conv_type\": \"GCN\",\n",
    "    \"gnn_layers\": 3,\n",
    "    \"dropout\": 0.5,        \n",
    "    \"lr\": 0.001,            \n",
    "    \"optimizer_lr\": 0.001,\n",
    "    \"lr_theta_temp\": 0.001,  \n",
    "    \"alpha\": 0.01,     \n",
    "   \n",
    "    \"loss\": \"CrossEntropyLoss\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"weight_decay\": 1e-3   # Added weight decay\n",
    "}\n",
    "\n",
    "\n",
    "model = GiGTrainer(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=500,\n",
    "    accelerator='gpu',\n",
    "    devices='auto',\n",
    "    # callbacks=callbacks,\n",
    "    logger=wandb_logger,\n",
    "    deterministic=False,\n",
    "    benchmark=True\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
